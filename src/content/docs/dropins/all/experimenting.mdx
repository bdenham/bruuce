---
title: Experimenting with drop-in components
description: Learn about how to create A/B experiments for drop-in components.
---

import { Tabs, TabItem } from '@astrojs/starlight/components';
import Diagram from '@components/Diagram.astro';
import Vocabulary from '@components/Vocabulary.astro';
import Aside from '@components/Aside.astro';
import Callouts from '@components/Callouts.astro';
import { Steps } from '@astrojs/starlight/components';
import Tasks from '@components/Tasks.astro';
import Task from '@components/Task.astro';

This topic introduces you to the concept of creating A/B experiments for drop-in components. You will learn how to create experiments, define variants, and track the performance of each variant.

## Big picture

<Diagram caption="What is an A/B experiment?">![What is an A/B experiment?](@images/placeholder.webp)</Diagram>

<Callouts>

1. **Experiment**: A test that compares two or more variants of a drop-in component to determine which variant performs better.
1. **Variant**: A version of a drop-in component that is different from the original version. Variants are created to test different features, designs, or content.
1. **Control**: The original version of the drop-in component that is used as a baseline for comparison.
1. **Challenger**: The variant of the drop-in component being tested.

</Callouts>

## Vocabulary

<Vocabulary>

### A/B experiment

A test that compares two or more pages to determine which page performs better.

### Variant

A version of a page that is different from the original version. Variants are created to test different features, designs, or content.

### Control

The original version of the drop-in component that is used as a baseline for comparison.

### Challenger

The variant of the drop-in component being tested.

</Vocabulary>

## Step-by-step

The steps for creating an A/B experiment for a commerce storefront page, are summarized below, and detailed in the following tasks:

<Steps>
1. Create a new variant page for the experiment.
1. Add the experiment to the metadata block of the original page.
1. Preview the experiment using AEM Sidekick Preview button.
</Steps>

<Tasks>
<Task>
### Create an experiment page

Duplicate the original page and make the necessary changes to create a new variant page for the experiment. Update any placeholder labels or content in the variant page to reflect the changes you want to test. For example, you can change the color of a button, update the text of a call-to-action, or rearrange the layout of the page. Save the variant page with a new name to distinguish it from the original page.

</Task>

<Task>
### Add the experiment to the original page

Add the experiment to the metadata block of the original page by adding the experiment name and the URL to the experiment page. For example:

| Metadata                      | |
| ------------------------ | -------------------------- |
| Title     | Cart      |
| Robots | noindex,nofollow |
| Experiment | CART001 |
| Experiment Variants | https://marketer-demo---aem-boilerplate-commerce--hlxsites.aem.page/drafts/marketer/cart-new |


</Task>
</Tasks>

## Track performance

After launching the experiment, you need to track the performance of each variant to determine which one performs better. Use analytics tools to monitor the KPIs and analyze the results. Based on the data, you can decide whether to implement the changes permanently or revert to the original version.

## Best practices

When creating A/B experiments for drop-in components, consider the following best practices:

- **Start small**: Test one variable at a time to isolate the impact of each change.
- **Use statistical significance**: Ensure that the sample size is large enough to draw valid conclusions.
- **Monitor continuously**: Track the performance of each variant throughout the experiment.
- **Iterate**: Use the results of the experiment to inform future tests and optimizations.

## Next steps

Now that you understand how to create A/B experiments for drop-in components, you can start experimenting with different variants to optimize the performance of your storefront. Remember to analyze the results and iterate on your experiments to drive continuous improvement.


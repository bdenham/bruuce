---
title: Experimenting with drop-in components
description: Learn about how to create A/B experiments for drop-in components.
---

import { Tabs, TabItem } from '@astrojs/starlight/components';
import Diagram from '@components/Diagram.astro';
import Vocabulary from '@components/Vocabulary.astro';
import Aside from '@components/Aside.astro';
import Callouts from '@components/Callouts.astro';
import { Steps } from '@astrojs/starlight/components';
import Tasks from '@components/Tasks.astro';
import Task from '@components/Task.astro';

This topic introduces you to the concept of creating A/B experiments for drop-in components. You will learn how to create experiments, define variants, and track the performance of each variant.

## Vocabulary

<Vocabulary>

### A/B experiment

A test that compares two or more pages to determine which page performs better.

### Variant

A version of a page that is different from the original version. Variants are created to test different features, designs, or content.

### UI label changes

Changes to the text or labels displayed on a page. Experimenting with label changes require you to create an alternate `placeholders` file and make minor changes to the drop-in's initializer.


### Control

The original version of the drop-in component that is used as a baseline for comparison.

### Challenger

The variant of the drop-in component being tested.

### Experimentation plugin

This plugin is included in the Commerce boilerplate ([hlxsites/aem-boilerplate-commerce](https://github.com/hlxsites/aem-boilerplate-commerce)) provides all the features for running A/B experiments on your storefront pages.

</Vocabulary>

## About storefront A/B experiments

The Experimentation plugin provides all the features for running A/B experiments on your storefront pages. You can track conversions and experiment with different page variations quickly, using real user metrics.

The features include setting up A/B tests, serving different content to multiple audiences, and integrating marketing campaigns without collecting personal user data. Metadata markers and rules define which versions of a page visitors see. By default, tests do not require user consent and don't require cookies.

Additionally, the plugin allows A/B experiments that can be run on mobile or desktop users and success can be tracked for each variant. You can also customize sample rates, data storage, and environment detection to suit your project. And with marketing campaigns, you can tailor experiences so visitors from a particular email or social link see unique content.

This plugin integrates seamlessly with the AEM boilerplate code, allowing experiments to be loaded early and with minimal impact.

For detailed development information, see https://github.com/hlxsites/aem-boilerplate-commerce/blob/main/plugins/experimentation/README.md.

## Step-by-step

Creating an A/B experiment for a commerce storefront page includes the following tasks:

<Steps>
1. Create a variant page for the experiment.
1. Add the experiment to the metadata block of the original page.
1. Preview the experiment using the AEM Sidekick extension.
1. Make code changes for experiments, if necessary.
</Steps>

<Tasks>
<Task>
### Create a variant page for the experiment

Duplicate the original page and make the changes you need to create a new page with the new content or features you want to use in the experiment. For most content changes, you can add those changes directly to the page/document. Examples include adding any of the [AEM content blocks](https://www.aem.live/developer/block-collection#boilerplate), additional commerce blocks (such as product recommendations) and more.

:::note
If you want to test changes to the UI labels in the drop-in, you can do that in code as part of the drop-in's initializer. See [Step 4: Code changes for experiments](#code-changes-for-experiments).
:::

</Task>

<Task>
### Add the experiment to the original page

Add the experiment to the metadata block of the original page by adding the experiment name and the URL to the experiment pages. The following example shows how to add an experiment to the cart drop-in block using the `metadata` block on the cart page. The keys are:

- **Experiment**: The name of the experiment. The Experimentation plugin uses this name to track this experiment.
- **Experiment Variants**: The URLs to the variant/challenger pages in the experiment. The Experimentation plugin uses these URLs to set up the experiment.

<Diagram caption="Add experiment to metadata block">![Add experiment to metadata block](@images/MetadataExperiment.png)</Diagram>

</Task>

<Task>
### Preview the experiment

Use the AEM Sidekick Preview button to preview the experiment and compare the original page with the variant page. Make sure that the changes are displayed correctly and that the experiment is functioning as expected. You can also share the experiment URL with stakeholders for feedback before launching it to a wider audience.

<Diagram caption="Preview the experiment">![Preview the experiment](@images/ExperimentOne.png)</Diagram>

</Task>

<Task>
### Code changes for experiments

Some experiments, such as creating variants of labels with the `placeholders` file, require code changes to a drop-in component's initializer code. For example, by making the following additions to the default `cart.js` initializer, you can read the metadata block from the cart page and identify when a variant is being delivered, then apply an alternate `placeholders` file.

<Diagram caption="Alternate placeholders file with different labels">![Alternate placeholders file with different labels](@images/PlaceholderChanges.png)</Diagram>

```js
/* eslint-disable import/no-cycle */
import { initializers } from '@dropins/tools/initializer.js';
import { initialize } from '@dropins/storefront-cart/api.js';
import { initializeDropin } from './index.js';
import { fetchPlaceholders } from '../aem.js';

await initializeDropin(async () => {

  // Identify experiment page (Challenger 1)
  const isVariantPlaceholder = document.body.classList.contains('experiment-cart001') && document.body.classList.contains('variant-challenger-1');

  // Fetch labels from an alternate placeholders file
  const labels = await fetchPlaceholders(isVariantPlaceholder ? '/drafts/marketer' : undefined);

  const langDefinitions = {
    default: {
      ...labels,
    },
  };

  return initializers.mountImmediately(initialize, { langDefinitions });
})();
```

</Task>
</Tasks>

## Next steps

Now that you understand how to create A/B experiments for drop-in components, you can start experimenting with different variants to optimize the performance of your storefront.
